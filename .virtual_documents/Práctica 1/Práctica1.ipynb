


# Importar librerías necesarias
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
df = pd.read_csv('titanic.csv')
df_cleaned = df.dropna().drop_duplicates()
df_cleaned.reset_index(drop=True, inplace=True)
filas_eliminadas = len(df) - len(df_cleaned)
print(f'Filas eliminadas: {filas_eliminadas}')


# 2. Determinar atributos no útiles
atributos_no_utiles = ['Ticket', 'PassengerId', 'Cabin', 'Name']





# 3. Relaciones entre atributos
sns.pairplot(df_cleaned)
plt.show()






# Seleccionar atributos numéricos
num_attrs = ['Age', 'SibSp', 'Parch', 'Fare']

# Diagramas de dispersión
sns.pairplot(df_cleaned[num_attrs])
plt.show()

# Matriz de correlación
corr_matrix = df_cleaned[num_attrs].corr()
print(corr_matrix)


# 4. Atributos numéricos
estadisticas = df_cleaned.describe()
print(estadisticas)


# 5. Atributos categóricos
categorical_columns = ['Sex', 'Embarked', 'Pclass']
for column in categorical_columns:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df_cleaned, x=column)
    plt.title(f'Frecuencia de {column}')
    plt.show()
    print(f'{column}:')
    print(f'Valores distintos: {df_cleaned[column].nunique()}')
    print(f'Valor más frecuente: {df_cleaned[column].mode()[0]}')





# 6. Determinar outliers
numerical_columns = ['Age', 'Fare', 'SibSp', 'Parch']
for column in numerical_columns:
    # Cálculo numérico de los outliers
    q1, q3 = df_cleaned[column].quantile([0.25, 0.75]) # Sacamos los quartiles 1 y 3
    IQR = q3 - q1 # Calculamos el rango entre quartiles (IQR)
    lowest_inlier = q1 - 1.5 * q1 # Usamos la fórmula para sacar el mínimo valor no-outlier
    highest_inlier = q3 + 1.5 * q3 # Y hacemos lo mismo para el máximo valor no-outlier
    # Contamos la cantidad de elementos que no estan en el rango y lo mostramos por pantalla
    outliers = 0 
    outliers += df_cleaned[column][df_cleaned[column] < lowest_inlier].count()
    outliers += df_cleaned[column][df_cleaned[column] > highest_inlier].count()
    print(f'{column} tiene {outliers} outliers.')
    # Representación en BoxPlot: Los círculos son los outliers
    plt.figure(figsize=(10, 5))
    sns.boxplot(data=df_cleaned, x=column)
    plt.title(f'Boxplot de {column}')
    plt.show()


# 7. Convertir atributos categóricos en valores numéricos
df_onehot = pd.get_dummies(df_cleaned, columns=categorical_columns)
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    df_cleaned[column] = le.fit_transform(df_cleaned[column])
    label_encoders[column] = le






# Mostramos los resultados del get_dummy
df_onehot.head()


# Mostramos los resultados del label_encoder
df_cleaned.head()


# 8. Normalizar y estandarizar el dataset
scaler_minmax = MinMaxScaler()
scaler_standard = StandardScaler()


# MinMaxScaler
df_numeric = df_cleaned.select_dtypes(include='number')
df_normalized = pd.DataFrame(
    scaler_minmax.fit_transform(df_numeric),
    columns=df_numeric.columns
)


# StandardScaler
df_standardized = pd.DataFrame(
    scaler_standard.fit_transform(),
    columns=df_numeric.columns
)


# Mostrar resultados
print("Normalización (MinMaxScaler):")
print(df_normalized.head())


print("\nEstandarización (StandardScaler):")
print(df_standardized.head())





  
from surprise import Dataset, Reader, KNNBasic, SVD, NMF
from surprise.model_selection import train_test_split
from surprise import accuracy


# Definir SEED
SEED = 42


# Cargar el dataset de MovieLens de 100K
data = Dataset.load_builtin('ml-100k')


# Dividir el dataset
trainset, testset = train_test_split(data, test_size=0.75, random_state=SEED)


# Evaluar distintos algoritmos de recomendación
# Filtrado colaborativo basado en vecinos (KNNBasic)
algo_knn_user = KNNBasic(sim_options={'name': 'pearson', 'user_based': True}, random_state=SEED)
algo_knn_item = KNNBasic(sim_options={'name': 'pearson', 'user_based': False}, random_state=SEED)


# Filtrado colaborativo basado en modelos (SVD y NMF)
algo_svd = SVD(random_state=SEED)
algo_nmf = NMF(random_state=SEED)


# Entrenar y evaluar los algoritmos
algorithms = [("KNN-User-Based", algo_knn_user),
              ("KNN-Item-Based", algo_knn_item),
              ("SVD", algo_svd),
              ("NMF", algo_nmf)]


# Mostrar resultados de 5 predicciones para cada algoritmo
for name, algo in algorithms:
    print(f"Evaluando {name}...")
    algo.fit(trainset)
    predictions = algo.test(testset)
    rmse = accuracy.rmse(predictions)
    print(f"{name} RMSE: {rmse}")

    print("Algunas predicciones:")
    for pred in predictions[:5]:
        print(pred)
    print()


from sklearn.metrics import precision_score, recall_score, ndcg_score
import numpy as np
def evaluate_metrics(predictions, k=10):
    # Convertir predicciones a un formato manejable
    relevant_items = []
    recommended_items = []
    scores = []

    for uid, iid, true_r, est, details in predictions:
        relevant_items.append(1 if true_r > 4 else 0)  # 1: relevante, 0: no relevante
        scores.append(est)

    # Obtener índices de las top-k recomendaciones
    sorted_indices = np.argsort(scores)[::-1][:k]
    recommended_items = [1 if i in sorted_indices else 0 for i in range(len(scores))]

    # Calcular Precision@k, Recall@k, NDCG@k
    precision = precision_score(relevant_items, recommended_items, zero_division=1)
    recall = recall_score(relevant_items, recommended_items, zero_division=1)
    ndcg = ndcg_score([relevant_items], [scores], k=k)

    return precision, recall, ndcg


# Evaluar cada algoritmo
results = []
for name, algo in algorithms:
    algo.fit(trainset)
    predictions = algo.test(testset)
    rmse = accuracy.rmse(predictions, verbose=False)
    precision, recall, ndcg = evaluate_metrics(predictions, k=10)

    results.append({
        "Modelo": name,
        "RMSE": rmse,
        "Precision@10": precision,
        "Recall@10": recall,
        "NDCG@10": ndcg
    })
import pandas as pd


# Crear un DataFrame a partir de los resultados
results_df = pd.DataFrame(results)


# Mostrar la tabla
print(results_df)




